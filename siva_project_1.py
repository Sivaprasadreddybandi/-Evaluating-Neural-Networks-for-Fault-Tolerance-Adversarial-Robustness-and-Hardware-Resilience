# -*- coding: utf-8 -*-
"""SIVA PROJECT 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cwVewuQexpqfQdpTukjOugtBhH0_tW3s
"""

!pip install timm
!pip install captum  # Run this if Captum is not installed
!pip install netcal

# Import required libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import shap
import gym
import time
from torchvision.models import resnet18
from timm import create_model
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from scipy.stats import entropy
import random
import os
import torch.nn.functional as F
import torch.nn.functional as F
import matplotlib.pyplot as plt
from captum.attr import LayerGradCam
from torchvision.transforms.functional import to_pil_image
import multiprocessing
from torchvision.models import resnet18, ResNet18_Weights
from torchvision.models import swin_t, Swin_T_Weights
from torchvision.models import efficientnet_b0
from torchvision.models import mobilenet_v2
from torchvision.models import densenet121
from captum.attr import LayerGradCam
from torchvision.transforms.functional import to_pil_image
from scipy.stats import entropy
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix
import torchvision.transforms.functional as TF
from PIL import Image
import numpy as np
import torch.nn.functional as F
from scipy.stats import entropy
import torch.nn.functional as F
from netcal.metrics import ECE
import numpy as np
import torch
import numpy as np
import torch
import torch.nn.functional as F
from scipy.stats import entropy
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from scipy.stats import entropy
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity


# Set device (use GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
print(f"Using device: {device}")
# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)
# Ensure MNIST dataset is stored in local Colab storage
root_dir = "/content/data"
os.makedirs(root_dir, exist_ok=True)
torch.backends.cudnn.benchmark = torch.cuda.is_available()  # Optimize GPU performance if CUDA is available
print(f"Using device: {device}")

# Optimized Data Loading
def get_dataloaders(batch_size=8):
    num_workers = min(4, multiprocessing.cpu_count())

    transform = transforms.Compose([
        transforms.Resize((28, 28)),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    trainset = torchvision.datasets.MNIST(root="./data", train=True, download=True, transform=transform)
    testset = torchvision.datasets.MNIST(root="./data", train=False, download=True, transform=transform)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    return trainloader, testloader

class Perceptron(nn.Module):
    def __init__(self):
        super(Perceptron, self).__init__()
        self.fc = nn.Linear(28*28, 10)

    def forward(self, x):
        x = x.view(x.size(0), -1)
        return self.fc(x)

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(8 * 14 * 14, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = x.view(-1, 8 * 14 * 14)
        return self.fc1(x)


class ResNet18Model(nn.Module):
    def __init__(self, use_pretrained=False):
        super(ResNet18Model, self).__init__()
        weights = ResNet18_Weights.DEFAULT if use_pretrained else None
        self.model = resnet18(weights=weights)
        self.model.fc = nn.Linear(self.model.fc.in_features, 10)

    def forward(self, x):
        if x.shape[1] == 1:
            x = x.repeat(1, 3, 1, 1)  # Convert grayscale to RGB

        x = x.requires_grad_()  # Ensure input requires gradients
        return self.model(x)

class SwinTransformerModel(nn.Module):
    def __init__(self, use_pretrained=False):
        super(SwinTransformerModel, self).__init__()
        self.model = swin_t(weights=None)
        self.model.head = nn.Linear(self.model.head.in_features, 10)

    def forward(self, x):
        if x.shape[1] == 1:
            x = x.repeat(1, 3, 1, 1)

        x = x.requires_grad_()  #Ensure input requires gradients
        return self.model(x)


# EfficientNet Model
class EfficientNetModel(nn.Module):
    def __init__(self, use_pretrained=False):
        super(EfficientNetModel, self).__init__()
        self.model = efficientnet_b0(weights=None)
        self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, 10)

    def forward(self, x):
        if x.shape[1] == 1:
            x = x.repeat(1, 3, 1, 1)

        x = x.requires_grad_()  # Ensure input requires gradients
        return self.model(x)


# MobileNetV2 Model
class MobileNetV2Model(nn.Module):
    def __init__(self, use_pretrained=False):
        super(MobileNetV2Model, self).__init__()
        self.model = mobilenet_v2(weights=None)
        self.model.classifier[1] = nn.Linear(self.model.classifier[1].in_features, 10)

    def forward(self, x):
        if x.shape[1] == 1:
            x = x.repeat(1, 3, 1, 1)  # Convert grayscale to RGB

        x = x.requires_grad_()  # Ensure input requires gradients
        return self.model(x)




# Function to Induce Bit-Flip Errors in Activations (Memory-Related Errors)
def induce_bitflip_activation_error(activation, fraction=0.05):
    with torch.no_grad():
        activation_clone = activation.clone()
        size = activation_clone.numel()
        num_errors = int(fraction * size)
        indices = np.random.choice(size, num_errors, replace=False)

        # Convert activation tensor to integers, flip bits, and convert back
        activation_flat = activation_clone.view(-1)
        activation_flat[indices] = torch.bitwise_not(activation_flat[indices].to(torch.int32)).to(torch.float32)

    return activation_clone

# Function to Detect & Correct Errors
def detect_and_correct_errors(activation, threshold=0.1):
    with torch.no_grad():
        activation_clone = activation.clone()
        mean_value = activation_clone.mean()
        mask = torch.abs(activation_clone) > threshold  # Detect outlier activations
        activation_clone[mask] = mean_value  # Correct outliers with mean value

    return activation_clone

def fine_tune_model(model, trainloader):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    model.train()
    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        break  # Only 1 mini-batch for quick fine-tuning

def induce_neuron_output_error(activation, fraction=0.1, noise_std=0.1, allow_gradients=True):
    if not allow_gradients:
        with torch.no_grad():
            return _apply_neuron_errors(activation.clone(), fraction, noise_std)

    return _apply_neuron_errors(activation.clone(), fraction, noise_std)


def _apply_neuron_errors(activation, fraction, noise_std):
    activation_clone = activation.clone()
    size = activation_clone.numel()
    num_errors = int(fraction * size)
    indices = np.random.choice(size, num_errors, replace=False)

    activation_flat = activation_clone.view(-1)  # Flatten tensor
    device = activation.device  # Ensure device consistency

    # Apply errors
    random_values = torch.rand(num_errors)  # Random values for selection
    mask_zero = random_values < 0.5  # 50% chance to set to zero
    mask_noise = ~mask_zero  # Remaining 50% add noise

    # Set zeroed activations
    activation_flat[indices[mask_zero]] = 0

    # Add Gaussian noise to selected neurons
    activation_flat[indices[mask_noise]] += torch.randn_like(activation_flat[indices[mask_noise]]) * noise_std

    return activation_clone

# Error Injection Functions
def induce_progressive_error(model, max_fraction=0.3, steps=5, delay=1.0):
    base_state = model.state_dict()  # Save original state
    with torch.no_grad():
        for step in range(1, steps + 1):
            error_fraction = (step / steps) * max_fraction
            print(f"\nStep {step}/{steps}: Injecting {error_fraction:.2%} error")
            for name, param in model.named_parameters():
                if "weight" in name:
                    size = param.numel()
                    num_errors = int(error_fraction * size)
                    indices = np.random.choice(size, num_errors, replace=False)
                    param.view(-1)[indices] = torch.randn(num_errors).to(param.device) * 0.1
            time.sleep(delay)
    model.load_state_dict(base_state)  # Restore original weights

def induce_error(model, fraction=0.1):
    with torch.no_grad():
        for name, param in model.named_parameters():
            if "weight" in name:
                size = param.numel()
                num_zeros = int(fraction * size)
                indices = np.random.choice(size, num_zeros, replace=False)
                param.view(-1)[indices] = 0

def induce_hybrid_error(model, bitflip_fraction=0.05, noise_fraction=0.1, noise_std=0.1):
    with torch.no_grad():
        for name, param in model.named_parameters():
            if "weight" in name:
                size = param.numel()

                # Bit-flip error
                num_bitflips = int(bitflip_fraction * size)
                bitflip_indices = np.random.choice(size, num_bitflips, replace=False)
                param_flattened = param.view(-1)  # Flatten tensor
                param_flattened[bitflip_indices] = torch.bitwise_not(param_flattened[bitflip_indices].to(torch.int32)).to(torch.float32)

                # Noise injection
                num_noise = int(noise_fraction * size)
                noise_indices = np.random.choice(size, num_noise, replace=False)
                noise = torch.randn(num_noise).to(param.device) * noise_std
                param_flattened[noise_indices] += noise

# Monte Carlo Simulation
def monte_carlo_simulation(model, testloader, num_simulations=10, error_fraction=0.1):
    accuracies = []
    base_model_state = model.state_dict()

    with torch.no_grad():
        for _ in range(num_simulations):
            model.load_state_dict(base_model_state)
            induce_error(model, fraction=error_fraction)
            acc = evaluate_model(model, testloader)
            accuracies.append(acc)

    return accuracies

# Function to verify if error injection is modifying model weights (without print)
def verify_error_injection(model, error_fraction=0.1):
    for name, param in model.named_parameters():
        if "weight" in name:  # Dynamically find the first weight layer
            original_weights = param.clone().detach()

            # Inject error
            induce_error(model, fraction=error_fraction)

            # Check weight differences
            modified_weights = param.clone().detach()
            diff = torch.sum(original_weights != modified_weights).item()

            return diff > 0  # Return True if weights changed, else False

    return False  # If no weight layer is found

# Function to compute KL-Divergence
def compute_kl_divergence(original_probs, perturbed_probs):
    original_probs += 1e-10  # Prevent log(0)
    perturbed_probs += 1e-10
    return entropy(original_probs, perturbed_probs)

# Function to analyze KL-Divergence impact with error injection
def kl_divergence_analysis(model, testloader, error_levels=[0, 0.05, 0.1, 0.2, 0.3, 0.5]):
    images, labels = next(iter(testloader))
    images, labels = images.to(device), labels.to(device)

    # Compute probability distributions before error injection
    outputs_before = model(images)
    probs_before = F.softmax(outputs_before, dim=1).cpu().detach().numpy()
    mean_prob_before = np.mean(probs_before, axis=0)  # Average over batch

    results = []

    for error_fraction in error_levels:
        induce_error(model, fraction=error_fraction)

        # Compute probability distributions after error injection
        outputs_after = model(images)
        probs_after = F.softmax(outputs_after, dim=1).cpu().detach().numpy()
        mean_prob_after = np.mean(probs_after, axis=0)

        # Compute KL divergence
        kl_div = compute_kl_divergence(mean_prob_before, mean_prob_after)
        print(f"Error Level: {error_fraction:.2%} | KL Divergence = {kl_div:.4f}")

        results.append((error_fraction, kl_div))

    return results

# Function to visualize KL-Divergence over increasing error levels
def plot_kl_divergence(results):
    error_levels, kl_values = zip(*results)

    plt.figure(figsize=(8, 5))
    plt.bar(error_levels, kl_values, color='purple', alpha=0.7)
    plt.xlabel("Error Injection Fraction")
    plt.ylabel("KL Divergence")
    plt.title("KL Divergence vs. Increasing Error Levels")
    plt.grid()
    plt.show()

from netcal.metrics import ECE

def expected_calibration_error(model, testloader, num_bins=10):
    # Load a batch of test images
    images, labels = next(iter(testloader))
    images, labels = images.to(device), labels.to(device)

    # Get model predictions
    outputs = model(images)
    probs = F.softmax(outputs, dim=1).cpu().detach().numpy()
    confidences = np.max(probs, axis=1)
    predictions = np.argmax(probs, axis=1)
    accuracies = (predictions == labels.cpu().numpy()).astype(float)

    # Correct initialization of `ECE`
    ece_metric = ECE(num_bins)  # Remove `n_bins=`

    # Compute ECE value
    ece_value = ece_metric.measure(confidences, accuracies)

    # Print ECE value
    print(f"Expected Calibration Error (ECE): {ece_value:.4f}")

    # Plot Reliability Diagram
    bin_boundaries = np.linspace(0, 1, num_bins + 1)
    bin_accs, bin_confs = [], []

    for i in range(num_bins):
        mask = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i + 1])
        if np.sum(mask) > 0:
            bin_accs.append(np.mean(accuracies[mask]))
            bin_confs.append(np.mean(confidences[mask]))
        else:
            bin_accs.append(0)
            bin_confs.append(0)

    plt.figure(figsize=(6, 6))
    plt.plot([0, 1], [0, 1], linestyle="--", color="gray", label="Perfect Calibration")
    plt.plot(bin_confs, bin_accs, marker="o", linestyle="-", color="blue", label="Model Calibration")
    plt.fill_between(bin_confs, bin_accs, bin_confs, color="red", alpha=0.2, label="ECE Gap")
    plt.xlabel("Confidence")
    plt.ylabel("Accuracy")
    plt.title(f"Reliability Diagram (ECE = {ece_value:.4f})")
    plt.legend()
    plt.grid()
    plt.show()

    return ece_value


# Decision Boundary Shift Visualization
def visualize_decision_boundary(model, testloader):
    images, labels = next(iter(testloader))
    images, labels = images.to(device), labels.to(device)

    outputs_before = model(images).cpu().detach().numpy()
    induce_error(model, fraction=0.1)
    outputs_after = model(images).cpu().detach().numpy()

    pca = PCA(n_components=2)
    reduced_before = pca.fit_transform(outputs_before)
    reduced_after = pca.transform(outputs_after)

    plt.figure(figsize=(8, 6))
    plt.scatter(reduced_before[:, 0], reduced_before[:, 1], c=labels.cpu().numpy(), cmap="coolwarm", alpha=0.5, label="Before Errors")
    plt.scatter(reduced_after[:, 0], reduced_after[:, 1], c=labels.cpu().numpy(), cmap="coolwarm", alpha=0.2, label="After Errors", marker="x")
    plt.colorbar()
    plt.title("PCA Decision Boundary Shift Due to Errors")
    plt.legend()
    plt.show()

def layer_sensitivity_analysis(model, testloader, error_fraction=0.1, layer_limit=10):
    base_model_state = model.state_dict()  # Save original state
    sensitivities = {}

    layer_count = 0  # Track the number of layers analyzed

    with torch.no_grad():
        for name, param in model.named_parameters():
            if "weight" in name:
                layer_count += 1
                if layer_count > layer_limit:  # Stop after analyzing `layer_limit` layers
                    break

                model.load_state_dict(base_model_state)  # Reset model before corruption

                acc_before = evaluate_model(model, testloader)  # Get accuracy before corruption
                print(f"Layer {name}: Accuracy before corruption = {acc_before:.4f}")

                # Corrupt weights
                size = param.numel()
                num_errors = int(error_fraction * size)
                indices = np.random.choice(size, num_errors, replace=False)
                param.view(-1)[indices] = 0  # Inject errors

                acc_after = evaluate_model(model, testloader)  # Get accuracy after corruption
                sensitivities[name] = acc_after

                print(f"Layer {name}: Accuracy after corruption = {acc_after:.4f}")

    model.load_state_dict(base_model_state)  # Restore original model weights

    # Plot layer sensitivity
    plt.figure(figsize=(10, 5))
    plt.bar(sensitivities.keys(), sensitivities.values(), color='red', alpha=0.7)
    plt.xticks(rotation=90)
    plt.xlabel("Layer Name")
    plt.ylabel("Accuracy After Corruption")
    plt.title(f"Layer Sensitivity (Top {layer_limit} Layers)")
    plt.grid()
    plt.show()

    return sensitivities


def verify_error_injection(model, error_fraction=0.1):
    for name, param in model.named_parameters():
        if "weight" in name:  # Find the first weight layer dynamically
            original_weights = param.clone().detach()

            # Inject error
            induce_error(model, fraction=error_fraction)

            # Check weight differences
            modified_weights = param.clone().detach()
            diff = torch.sum(original_weights != modified_weights).item()

            # Remove print statement
            return diff > 0  # Stop after checking first weight layer

    return False  # If no weight layer is found

# Function to compute entropy
def compute_entropy(probabilities):
    return np.mean([entropy(prob) for prob in probabilities])

# Function to compute Mean Absolute Difference (MAD)
def compute_mad(prob1, prob2):
    return np.mean(np.abs(prob1 - prob2))

# Function to compute Cosine Similarity between probability distributions
def compute_cosine_similarity(original_probs, perturbed_probs):
    return cosine_similarity(original_probs.reshape(1, -1), perturbed_probs.reshape(1, -1))[0][0]

# Stronger Error Injection to ensure changes
def induce_error(model, fraction=0.1, noise_std=0.05):
    with torch.no_grad():
        for name, param in model.named_parameters():
            if "weight" in name:
                size = param.numel()
                num_errors = int(fraction * size)
                indices = np.random.choice(size, num_errors, replace=False)
                param.view(-1)[indices] += torch.randn(num_errors).to(param.device) * noise_std  # Add noise

# Function to analyze uncertainty with entropy, MAD, and Cosine Similarity
def uncertainty_analysis(model, testloader, error_levels=[0, 0.05, 0.1, 0.2, 0.3, 0.5]):
    images, labels = next(iter(testloader))
    images, labels = images.to(device), labels.to(device)

    # Compute baseline entropy & probabilities before error injection
    outputs_before = model(images)
    probs_before = F.softmax(outputs_before, dim=1).cpu().detach().numpy()
    entropy_before = compute_entropy(probs_before)

    print("Uncertainty Analysis:")
    print("------------------------------------------------")

    results = []

    for error_fraction in error_levels:
        induce_error(model, fraction=error_fraction)

        # Compute probabilities & entropy after error injection
        outputs_after = model(images)
        probs_after = F.softmax(outputs_after, dim=1).cpu().detach().numpy()
        entropy_after = compute_entropy(probs_after)
        mad_value = compute_mad(probs_before, probs_after)
        cosine_sim = compute_cosine_similarity(probs_before, probs_after)

        print(f"Error Level: {error_fraction:.2%} | Entropy Before = {entropy_before:.4f} | "
              f"Entropy After = {entropy_after:.4f} | MAD Value = {mad_value:.4f} | "
              f"Cosine Similarity = {cosine_sim:.4f}")

        results.append((error_fraction, entropy_before, entropy_after, mad_value, cosine_sim))

    return results

# Function to visualize uncertainty analysis results
def plot_uncertainty_analysis(results):
    df = pd.DataFrame(results, columns=["Error Fraction", "Entropy Before", "Entropy After", "MAD Value", "Cosine Similarity"])

    fig, ax = plt.subplots(1, 3, figsize=(15, 5))

    # Bar plot for entropy shift
    ax[0].bar(df["Error Fraction"], df["Entropy Before"], width=0.02, label="Entropy Before", color='blue', alpha=0.6)
    ax[0].bar(df["Error Fraction"], df["Entropy After"], width=0.02, label="Entropy After", color='red', alpha=0.6)
    ax[0].set_xlabel("Error Injection Fraction")
    ax[0].set_ylabel("Mean Entropy (Uncertainty)")
    ax[0].set_title("Entropy Shift vs. Error Levels")
    ax[0].legend()
    ax[0].grid()

    # Line plot for MAD shift
    ax[1].plot(df["Error Fraction"], df["MAD Value"], marker='o', linestyle='-', color='purple', label="MAD")
    ax[1].set_xlabel("Error Injection Fraction")
    ax[1].set_ylabel("Mean Absolute Difference (MAD)")
    ax[1].set_title("MAD Shift vs. Error Levels")
    ax[1].legend()
    ax[1].grid()
    ax[1].set_ylim(0, max(df["MAD Value"]) + 0.01)  # Force y-axis range

    # Heatmap for uncertainty metrics
    plt.figure(figsize=(8, 5))
    sns.heatmap(df.drop("Error Fraction", axis=1), annot=True, cmap="coolwarm", xticklabels=["Entropy Before", "Entropy After", "MAD", "Cosine Sim"])
    plt.title("Heatmap of Uncertainty Metrics Across Error Levels")
    plt.show()

# Increase batch size for better entropy variation
trainloader, testloader = get_dataloaders(batch_size=8)  # Lower batch size for memory safety

def fgsm_attack(model, images, labels, epsilon=0.1):
    images = images.clone().detach().to(device)  # Ensure fresh copies
    labels = labels.clone().detach().to(device)
    images.requires_grad = True

    outputs = model(images)
    loss = F.cross_entropy(outputs, labels)

    model.zero_grad()
    loss.backward()

    perturbation = epsilon * images.grad.sign()
    perturbed_images = torch.clamp(images + perturbation, 0, 1)  # Keep pixel values valid

    return perturbed_images.detach()

def visualize_fgsm_attack(model, dataloader, epsilon=0.1):
    # Reload a new test batch for every model
    test_images, test_labels = next(iter(dataloader))
    test_images, test_labels = test_images.to(device), test_labels.to(device)

    # Generate adversarial examples
    adv_images = fgsm_attack(model, test_images, test_labels, epsilon)

    # Plot original vs adversarial images
    fig, axes = plt.subplots(5, 2, figsize=(10, 10))

    for i in range(5):  # Show first 5 images
        original_img = test_images[i].cpu().detach().squeeze().numpy()
        adversarial_img = adv_images[i].cpu().detach().squeeze().numpy()

        axes[i, 0].imshow(original_img, cmap="gray")
        axes[i, 0].set_title(f"Original: {test_labels[i].item()}")
        axes[i, 0].axis("off")

        axes[i, 1].imshow(adversarial_img, cmap="gray")
        axes[i, 1].set_title(f"Adversarial (ε={epsilon})")
        axes[i, 1].axis("off")

    plt.tight_layout()
    plt.show()

def adversarial_noise_injection(model, images, labels, epsilon=0.1, bitflip_fraction=0.05):
    perturbed_images = fgsm_attack(model, images, labels, epsilon=epsilon)

    with torch.no_grad():
        for name, param in model.named_parameters():
            if "weight" in name and param.requires_grad:
                size = param.numel()
                num_bitflips = int(bitflip_fraction * size)
                indices = np.random.choice(size, num_bitflips, replace=False)

                # Ensure contiguous tensor
                param_flattened = param.flatten()
                param_flattened[indices] = -param_flattened[indices]

    return perturbed_images

# PGD Attack
def pgd_attack(model, images, labels, epsilon=0.1, alpha=0.01, iters=40):
    perturbed_images = images.clone().detach().to(device)  # Ensure it's on the correct device
    perturbed_images.requires_grad = True  # Set requires_grad to True for gradient computation
    labels = labels.to(device)

    # Random initialization for perturbation within the range of epsilon
    perturbed_images = perturbed_images + torch.empty_like(perturbed_images).uniform_(-epsilon, epsilon)
    perturbed_images = torch.clamp(perturbed_images, 0, 1)  # Keep values in valid range

    for _ in range(iters):
        # Create a fresh tensor for perturbed images on each iteration with requires_grad=True
        perturbed_images = perturbed_images.clone().detach().to(device)
        perturbed_images.requires_grad = True

        # Compute the loss and gradients
        outputs = model(perturbed_images)
        loss = F.cross_entropy(outputs, labels)

        model.zero_grad()
        loss.backward()

        # Compute perturbation
        perturbation = alpha * perturbed_images.grad.sign()
        perturbed_images = perturbed_images + perturbation
        perturbed_images = torch.clamp(perturbed_images, images - epsilon, images + epsilon)  # Projection step
        perturbed_images = torch.clamp(perturbed_images, 0, 1)  # Keep values in range

    return perturbed_images.detach()  # Detach from computation graph

# Load a batch of test images
testloader = get_dataloaders(batch_size=8)[1]
batch = next(iter(testloader))

# Correctly unpack images and labels
if isinstance(batch, (list, tuple)) and len(batch) == 2:
    test_images, test_labels = batch  # Unpack images and labels correctly
else:
    raise ValueError("Unexpected batch format detected. Check the dataloader.")

# Move test images and labels to the correct device
test_images, test_labels = test_images.to(device), test_labels.to(device)

# Print final shape for verification
print(f"Images shape: {test_images.shape}")  # Expected: torch.Size([8, 1, 28, 28])

def shap_analysis(model, testloader, num_samples=10):
    model.eval()  # Ensure model is in evaluation mode
    images, labels = next(iter(testloader))
    images, labels = images.to(device), labels.to(device)

    # Ensure requires_grad is enabled for SHAP
    images.requires_grad = True

    # Ensure all model parameters require gradients
    for param in model.parameters():
        param.requires_grad = True

    # Create SHAP explainer
    explainer = shap.GradientExplainer(model, images)

    # Compute SHAP values before errors
    try:
        shap_values_before = explainer.shap_values(images)
    except RuntimeError as e:
        print("SHAP failed due to missing gradients. Retrying with allow_unused=True.")
        shap_values_before = explainer.shap_values(images, allow_unused=True)

    # Apply errors to the model
    model.apply(lambda module: setattr(module, 'allow_gradients', True))

    # Compute SHAP values after errors
    try:
        shap_values_after = explainer.shap_values(images)
    except RuntimeError as e:
        print("SHAP failed after error injection. Retrying with allow_unused=True.")
        shap_values_after = explainer.shap_values(images, allow_unused=True)

    # Select a single class to visualize (e.g., class 0)
    class_idx = 0
    shap_values_before_selected = shap_values_before[class_idx][0].squeeze()
    shap_values_after_selected = shap_values_after[class_idx][0].squeeze()

    # Ensure SHAP values have correct shape (28,28)
    if shap_values_before_selected.ndim == 3:
        shap_values_before_selected = shap_values_before_selected.mean(axis=0)
    if shap_values_after_selected.ndim == 3:
        shap_values_after_selected = shap_values_after_selected.mean(axis=0)

    # Visualize SHAP importance before and after errors
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.imshow(shap_values_before_selected, cmap="coolwarm")
    plt.title(f"SHAP Feature Importance (Before Errors) - Class {class_idx}")
    plt.colorbar()

    plt.subplot(1, 2, 2)
    plt.imshow(shap_values_after_selected, cmap="coolwarm")
    plt.title(f"SHAP Feature Importance (After Errors) - Class {class_idx}")
    plt.colorbar()

    plt.show()

def get_last_conv_layer(model):
    for layer in reversed(list(model.children())):
        if isinstance(layer, nn.Conv2d):  # Ensure it's a convolutional layer
            return layer
    return None  # Return None if no Conv2D layer is found

def get_last_conv_layer(model):
    for layer in reversed(list(model.children())):
        if isinstance(layer, nn.Conv2d):  # Ensure it's a convolutional layer
            return layer
    return None  # Return None if no Conv2D layer is found

def grad_cam_analysis(model, testloader):
    model.eval()
    images, labels = next(iter(testloader))
    images, labels = images.to(device), labels.to(device)

    # Select the first image for visualization
    image = images[0].unsqueeze(0)

    # Automatically find a convolutional layer for Grad-CAM
    target_layer = get_last_conv_layer(model)
    if target_layer is None:
        print(f"Grad-CAM skipped for {model.__class__.__name__} (No Conv2D layer found)")
        return

    print(f"Using {target_layer} for Grad-CAM")

    # Create Grad-CAM explainer
    grad_cam = LayerGradCam(model, target_layer)

    # Compute Grad-CAM before errors
    cam_before = grad_cam.attribute(image, target=int(labels[0]))

    # Apply errors to the model
    induce_error(model, fraction=0.1)

    # Compute Grad-CAM after errors
    cam_after = grad_cam.attribute(image, target=int(labels[0]))

    # Convert to NumPy for visualization and check shape
    cam_before_np = cam_before.squeeze().cpu().detach().numpy()
    cam_after_np = cam_after.squeeze().cpu().detach().numpy()

    if cam_before_np.ndim != 2:
        print("Warning: Grad-CAM output has an unexpected shape, skipping visualization.")
        return

    # Visualize Grad-CAM before and after errors
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(to_pil_image(images[0].cpu()), alpha=0.5)
    plt.imshow(cam_before_np, cmap="jet", alpha=0.5)
    plt.title("Grad-CAM (Before Errors)")
    plt.colorbar()
    plt.subplot(1, 2, 2)
    plt.imshow(to_pil_image(images[0].cpu()), alpha=0.5)
    plt.imshow(cam_after_np, cmap="jet", alpha=0.5)
    plt.title("Grad-CAM (After Errors)")
    plt.colorbar()
    plt.show()

def apply_corruptions(images):
    corrupted_images = []
    for img in images:
        corruption_type = random.choice(["noise", "blur", "compression", "occlusion"])

        img = img.clone()  # Create a copy before modifying

        if corruption_type == "noise":
            img = img + torch.randn_like(img) * 0.1  # Gaussian noise
        elif corruption_type == "blur":
            img = TF.gaussian_blur(img, kernel_size=5)  # Motion blur
        elif corruption_type == "compression":
            pil_img = TF.to_pil_image(img)  # Convert to PIL image
            pil_img = pil_img.convert("L")  # Convert back to grayscale (Ensures 1 channel)
            img = TF.to_tensor(pil_img)  # Convert back to tensor
        elif corruption_type == "occlusion":
            h, w = img.shape[1], img.shape[2]
            x, y = random.randint(0, w // 2), random.randint(0, h // 2)
            img[:, y:y + h // 4, x:x + w // 4] = 0  # Black patch occlusion Fixed

        corrupted_images.append(img)

    return torch.stack(corrupted_images).to(device)

# Function to Evaluate Model
def evaluate_model(model, testloader):
    correct, total = 0, 0
    model.eval()
    amp_enabled = torch.cuda.is_available()

    with torch.no_grad():
        with torch.amp.autocast(device_type="cuda") if amp_enabled else torch.no_grad():
            for images, labels in testloader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
    return correct / total

# Load test data
trainloader, testloader = get_dataloaders(batch_size=32)
images, labels = next(iter(testloader))
images, labels = images.to(device), labels.to(device)

# Run Full Experiment
models = {
    "Perceptron": Perceptron().to(device),
    "CNN": CNN().to(device),
    "ResNet18": ResNet18Model(use_pretrained=True).to(device),
    "Swin Transformer": SwinTransformerModel(use_pretrained=True).to(device),
    "EfficientNetModel": EfficientNetModel(use_pretrained=True).to(device),
    "MobileNetV2Model": MobileNetV2Model(use_pretrained=True).to(device)
}
for model_name, model in models.items():
    trainloader, testloader = get_dataloaders(batch_size=8)
    model.to(device)
    print(f"\n\033[1mEvaluating : {model_name}\033[0m")
    print("-----------------------------")
    acc_before = evaluate_model(model, testloader)
    induce_error(model, fraction=0.1)
    acc_after = evaluate_model(model, testloader)
    print(f"{model_name} - Accuracy before: {acc_before:.2%}, after: {acc_after:.2%}")
    induce_error(model, fraction=0.1)
    print(f"{model_name} - Accuracy after Error Injection: {evaluate_model(model, testloader):.2%}")
    induce_hybrid_error(model)
    print(f"{model_name} - Accuracy after Hybrid Error: {evaluate_model(model, testloader):.2%}")
    print("-----------------------------------------------------")
    induce_progressive_error(model)
    print(f"{model_name} - Accuracy after Progressive Error: {evaluate_model(model, testloader):.2%}")
    print("---------------------------------------------------------------------")
    print(f"Monte Carlo Simulation: ")
    num_simulations = 5 if model_name == "ViT" else 10  # Run fewer simulations for ViT
    start_time = time.time()
    accuracies = monte_carlo_simulation(model, testloader, num_simulations=num_simulations, error_fraction=0.1)
    end_time = time.time()
    print("------------------------------------------------")
    plt.hist(accuracies, bins=10, alpha=0.7, color='red')
    plt.xlabel("Accuracy")
    plt.ylabel("Frequency")
    plt.title(f"Monte Carlo Failure Distribution ({model_name})")
    plt.show()
    print("Decision Boundary Shift Visualization :")
    print("---------------------------------------")
    visualize_decision_boundary(model, testloader)
    if verify_error_injection(model, error_fraction=0.1):
      results = uncertainty_analysis(model, testloader)
      plot_uncertainty_analysis(results)
    else:
      print("Error Injection did NOT modify weights! Check `induce_error()` function.")
    print("-------------------------------------")
    print("Layer Senstivity Analysis :")
    print("---------------------------")
    layer_sensitivity_analysis(model, testloader, error_fraction=0.1, layer_limit=8)
    print("--------------------------------------------------------")
    print("Shap Analysis :")
    print("---------------")
    shap_analysis(model, testloader, num_samples=10)  # Uses error injection but retains gradients
    print("Grad Cam Analysis :")
    print("-------------------")
    grad_cam_analysis(model, testloader)
    print("-------------------------------------------------------")
    print("FGSM Attack Analysis :")
    print("----------------------")
    print(f"\nRunning FGSM Attack for {model_name}")
    # Generate adversarial images
    adv_images = fgsm_attack(model, test_images, test_labels, epsilon=0.1)
    # Evaluate model before and after FGSM attack
    acc_before_fgsm = evaluate_model(model, testloader)
    acc_after_fgsm = evaluate_model(model, [(adv_images, test_labels)])
    # Print accuracy drop due to FGSM attack
    print(f"{model_name} - Accuracy before FGSM attack: {acc_before_fgsm:.2%}")
    print(f"{model_name} - Accuracy after FGSM attack: {acc_after_fgsm:.2%}")
    # Visualize original vs. adversarial images
    visualize_fgsm_attack(model, testloader, epsilon=0.1)
    print("---------------------------------------")
    print("PGD Attack Analysis :")
    print("---------------------")
    # Evaluate the model on the test data
    acc_before = evaluate_model(model, testloader)
    print(f"{model_name} - Accuracy before PGD attack: {acc_before:.2%}")
    # PGD Attack: Generate adversarial examples
    adversarial_images = pgd_attack(model, test_images, test_labels, epsilon=0.1, alpha=0.01, iters=40)
    # Ensure adversarial images match the batch size of the labels
    if adversarial_images.size(0) != test_labels.size(0):
        raise ValueError(f"Adversarial images batch size {adversarial_images.size(0)} does not match labels batch size {test_labels.size(0)}.")
    # Evaluate the model on adversarial examples
    adversarial_images = adversarial_images.to(device)
    acc_after_pgd = evaluate_model(model, [(adversarial_images, test_labels)])  # Evaluate with adversarial data
    print(f"{model_name} - Accuracy after PGD attack: {acc_after_pgd:.2%}")
    print("---------------------------------------")
    print("Randomized Adversarial Noise Injection:")
    print(f"\nApplying Adversarial Noise Injection on {model_name}")
    print("-----------------------------------------------------")

    # Move model to the correct device
    model.to(device)
    model.eval()

    # Generate adversarial images with FGSM + Bit-Flip
    perturbed_images = adversarial_noise_injection(model, test_images, test_labels, epsilon=0.1, bitflip_fraction=0.05)

    # Evaluate model accuracy before and after attack
    acc_before = evaluate_model(model, testloader)
    acc_after = evaluate_model(model, [(perturbed_images, test_labels)])  # Evaluate with perturbed images

    print(f"{model_name} - Accuracy before attack: {acc_before:.2%}")
    print(f"{model_name} - Accuracy after attack: {acc_after:.2%}")

    # Visualization: Original vs Perturbed Images
    fig, axes = plt.subplots(2, 5, figsize=(12, 5))
    for i in range(5):
        axes[0, i].imshow(test_images[i].cpu().detach().squeeze(), cmap="gray")
        axes[0, i].set_title(f"Original: {test_labels[i].item()}")
        axes[0, i].axis("off")

        axes[1, i].imshow(perturbed_images[i].cpu().detach().squeeze(), cmap="gray")
        axes[1, i].set_title(f"Perturbed Image")
        axes[1, i].axis("off")

    plt.suptitle(f"Original vs. Adversarially Perturbed Images ({model_name})")
    plt.show()
    corrupted_images = apply_corruptions(images)
    corrupted_images = corrupted_images[:images.shape[0]]
    labels = labels[:corrupted_images.shape[0]]
    acc_clean = evaluate_model(model, [(images, labels)])
    acc_corrupted = evaluate_model(model, [(corrupted_images, labels)])
    print(f"{model_name} - Accuracy on Clean Data: {acc_clean:.2%}")
    print(f"{model_name} - Accuracy on Corrupted Data: {acc_corrupted:.2%}")

    print("---------------------------------------------------------------------")
    print("KL Divergence Analysis :")
    print("------------------------")
    if verify_error_injection(model, error_fraction=0.1):
      kl_results = kl_divergence_analysis(model, testloader)
      plot_kl_divergence(kl_results)
    else:
      print("Error Injection did NOT modify weights! Check `induce_error()` function.")


    print("Expected Calibration Error :")
    print("----------------------------")
    ece_value = expected_calibration_error(model, testloader)
    print("------------------------------------------------")

    print("Function to Induce Bit-Flip Errors in Activations :")
    acc_before = evaluate_model(model, testloader)
    print(f"Accuracy before memory error injection: {acc_before:.2%}")
    fine_tune_model(model, trainloader)
    print("---------------------------------------------------------")
    acc_after = evaluate_model(model, testloader)
    print("Fine-Tuning for Recovery")
    print("------------------------")
    print(f"Accuracy after fine-tuning (Recovery): {acc_after:.2%}")
    print("--------------------------------------------------------")
    print ("Plot Accuracy Before and After Recovery")
    print("------------------------------------------------")
    plt.bar(["Before Errors", "After Recovery"], [acc_before * 100, acc_after * 100], color=['red', 'green'])
    plt.ylabel("Accuracy (%)")
    plt.title(f"{model_name} - Recovery Analysis")
    plt.show()